{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d8c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5026dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70f214c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fc1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleProcessor():\n",
    "    \n",
    "    def __init__(self, priceFile, newsFile, tickerName, priceType):\n",
    "        self.tickerName = tickerName\n",
    "        self.priceType = priceType\n",
    "        self.news_data = pd.read_csv(newsFile, index_col = [0])\n",
    "        self.price_vol = pd.read_csv(priceFile, index_col= [0])\n",
    "        self.crypto_price = self.price_vol[[tickerName + '-' + priceType]].shift(1)\n",
    "        self.crypto_news = self.news_data[[tickerName]]\n",
    "        self.crypto_price.index = pd.to_datetime(self.crypto_price.index).tz_convert(None)\n",
    "        self.crypto_news.index = pd.to_datetime(self.crypto_news.index)\n",
    "        self.data = None\n",
    "        \n",
    "    def getData(self):\n",
    "        data = self.crypto_news.merge(self.crypto_price, how = 'inner', left_index = True, right_index = True)\n",
    "        data[self.tickerName + '_summary'] = data.apply(lambda x :self.joinStr(x), axis = 1)\n",
    "        data['returns'] = data[self.tickerName + '-' + self.priceType].pct_change().shift(-1)\n",
    "        data['class'] = data['returns'] > 0\n",
    "        data = data.iloc[:-1, :]\n",
    "        \n",
    "        nonewsdate = data[data.isna().any(axis = 1)].index\n",
    "        newsdate = data[~data.isna().any(axis = 1)].index\n",
    "        \n",
    "        self.data = data[~data.isna().any(axis = 1)][[self.tickerName + '_summary', 'class']]\n",
    "        self.data.columns = ['text', 'class']\n",
    "        self.data['class'] = self.data['class'].map({True : 1, False : 0})\n",
    "        \n",
    "        return self.data.copy()\n",
    "        \n",
    "    @staticmethod\n",
    "    def joinStr(df):\n",
    "        lists = eval(df[0])\n",
    "        if(lists[0]):\n",
    "            return ' '.join(lists[0])\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c00087",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = simpleProcessor('price_vol.csv', 'btc_eth.csv', 'BTC', 'close').getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a66a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-03-01 13:00:00</th>\n",
       "      <td>Geopolitical Risk Returns for Global Markets G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02 03:00:00</th>\n",
       "      <td>Asian shares slip, oil surges again as Russia ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02 08:00:00</th>\n",
       "      <td>Business Highlights: Lobbyists leaving, rate h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02 09:00:00</th>\n",
       "      <td>Business Highlights: Lobbyists leaving, rate h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02 11:00:00</th>\n",
       "      <td>Millions for Crypto Start-Ups, No Real Names N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  class\n",
       "2022-03-01 13:00:00  Geopolitical Risk Returns for Global Markets G...      1\n",
       "2022-03-02 03:00:00  Asian shares slip, oil surges again as Russia ...      1\n",
       "2022-03-02 08:00:00  Business Highlights: Lobbyists leaving, rate h...      1\n",
       "2022-03-02 09:00:00  Business Highlights: Lobbyists leaving, rate h...      1\n",
       "2022-03-02 11:00:00  Millions for Crypto Start-Ups, No Real Names N...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5453472f",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d381d",
   "metadata": {},
   "source": [
    "## Dataset Class Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d27061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class newsDataset(Dataset): \n",
    "    \n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.labels = [i for i in df['class']]\n",
    "        self.texts = [tokenizer(str(text), \n",
    "                                padding='max_length',\n",
    "                                truncation = True, \n",
    "                                return_tensors='pt')\n",
    "                      for text in df['text']]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "    \n",
    "    def get_batch_text(self, idx):\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_text(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        \n",
    "        return batch_texts, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43d91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.8\n",
    "n = train_data1.shape[0]\n",
    "trainData, testData = train_data1.iloc[0 : int(n*percent)], train_data1.iloc[int(n*percent):]\n",
    "m = trainData.shape[0]\n",
    "trainData, validData = trainData.iloc[0 : int(m*percent)], trainData.iloc[int(m*percent): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3b89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_NAME)\n",
    "trainDataLoader = DataLoader(newsDataset(trainData, tokenizer), batch_size = 2)\n",
    "validDataLoader = DataLoader(newsDataset(validData, tokenizer), batch_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72524eb",
   "metadata": {},
   "source": [
    "## Bert Classifier \n",
    "This is a simple pre-trained one in Huggingface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c455462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(): \n",
    "    \n",
    "    def __init__(self, bertname, lr, totalSteps):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "        bertname, \n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "        )\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr = lr, eps = 1e-8)\n",
    "        self.scheduler = get_scheduler(name = 'linear', optimizer=self.optimizer, num_warmup_steps=0, \\\n",
    "                                       num_training_steps=totalSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff84006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(clf, trainLoader, validLoader, epochs):\n",
    "    #epochs \n",
    "    for _ in trange(epochs):\n",
    "        \n",
    "        #set model to training model\n",
    "        clf.model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_num = 0\n",
    "        for train_inputs, train_labels in trainLoader:\n",
    "            \n",
    "            train_labels = train_labels.long()\n",
    "            train_labels = train_labels.to(clf.device)\n",
    "            mask = train_inputs['attention_mask'].to(clf.device)\n",
    "            input_id = train_inputs['input_ids'].squeeze(1).to(clf.device) \n",
    "            \n",
    "            clf.optimizer.zero_grad()\n",
    "\n",
    "            output = clf.model(input_id, attention_mask = mask, labels = train_labels)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "                        \n",
    "            clf.optimizer.step()\n",
    "            clf.scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_num += input_id.size()[0]\n",
    "            \n",
    "        # Validation\n",
    "        clf.model.eval()\n",
    "        \n",
    "        total_loss_val = 0\n",
    "        total_num_val = 0\n",
    "        preds = []\n",
    "        trues = []\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for valid_inputs, valid_labels in validLoader:\n",
    "                \n",
    "                valid_labels = valid_labels.long()\n",
    "                valid_labels = valid_labels.to(clf.device)\n",
    "                mask = valid_inputs['attention_mask'].to(clf.device)\n",
    "                input_id = valid_inputs['input_ids'].squeeze(1).to(clf.device)\n",
    "                \n",
    "                output = clf.model(input_id, attention_mask = mask, labels = valid_labels)\n",
    "                \n",
    "                loss = output.loss\n",
    "                total_loss_val += loss.item()\n",
    "                total_num_val += input_id.size()[0]\n",
    "\n",
    "                logits = output.logits.detach().cpu().numpy()\n",
    "                predictions = np.argmax(logits, axis = 1).flatten()\n",
    "                labels = valid_labels.detach().cpu().numpy().flatten()\n",
    "                \n",
    "                preds += list(predictions)\n",
    "                trues += list(labels)\n",
    "        \n",
    "        \n",
    "        print('\\n\\t - Train loss : {:.4f}'.format(total_loss / total_num))\n",
    "        print('\\t - Validation loss : {:.4f}'.format(total_loss_val / total_num_val))\n",
    "        print('\\t - Validation accuracy : {:.4f}'.format(accuracy_score(trues, preds)))\n",
    "        print('\\t - Validation precision : {:.4f}'.format(precision_score(trues, preds)))\n",
    "        print('\\t - Validation recall : {:.4f}'.format(recall_score(trues, preds)))\n",
    "    \n",
    "    return preds, trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d1b7b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "clf = BertBinaryClassifier('bert-base-uncased', lr = 2e-5, totalSteps=num_epochs*len(trainDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e175d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [11:28<00:00, 688.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss : 0.3495\n",
      "\t - Validation loss : 0.3473\n",
      "\t - Validation accuracy : 0.5086\n",
      "\t - Validation precision : 0.6304\n",
      "\t - Validation recall : 0.0887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds, trues = trainer(clf, trainDataLoader, validDataLoader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9dd0865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed044e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
